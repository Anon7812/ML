from scipy import stats
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error,r2_score
x=[0.5, 0.8, 1.1, 1.5, 1.9, 2.4, 2.8, 3.1, 3.5, 3.9]
y=[2.5, 3.1, 3.6, 4.2, 5.0, 6.1, 6.5, 7.0, 7.8, 8.3]
slope,intercept,r,p,std_err=stats.linregress(x,y)
mymodel=[slope*i + intercept for i in x]
plt.scatter(x,y)
plt.plot(x,mymodel)
plt.show()
print(f'Intercept: {intercept}')
print(r2_score(y,mymodel))
print(mean_squared_error(y,mymodel))
--------------------------------------------
import numpy as np
from sklearn.linear_model import LinearRegression

# Experiment data
diameters = [1, 2, 3, 1, 2, 3, 1, 2, 3]
slopes = [0.001, 0.001, 0.001, 0.01, 0.01, 0.01, 0.05, 0.05, 0.05]
flows = [1.4, 8.3, 24.2, 4.7, 28.9, 84.0, 11.1, 69.0, 200.0]

# Create a matrix of independent variables
X = np.column_stack((np.ones(len(diameters)), diameters, slopes))

# Fit the multiple linear regression model
model = LinearRegression().fit(X, flows)

# Coefficients of the regression model
a0 = model.intercept_
a1, a2 = model.coef_[1:]

# Print the results
print(f'a0 (intercept): {a0}')
print(f'a1 (coefficient for diameter): {a1}')
print(f'a2 (coefficient for slope): {a2}')

# Predict flow for a pipe with diameter 2.5ft and slope 0.025 ft/ft
predicted_flow = model.predict([[1, 2.5, 0.025]])
print(f'Predicted Flow for Diameter=2.5ft, Slope=0.025 ft/ft: {predicted_flow[0]} ft^3/s')
-------------------------------------------------
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
x=np.array([1,2,3,4,7,6,7,10,11,12])
y=np.array([1,3,6,5,11,10,12,15,16,15])
data=list(zip(x,y))
# data=pd.read_csv("data4_19.csv")
# attributes=data.iloc[:,:4].values
# labels=data.iloc[:,4].values
inertias=[]
for i in range(1,7):
    kmeans=KMeans(n_clusters=i,n_init=100)
    kmeans.fit(data)
    inertias.append(kmeans.inertia_)
# plt.scatter(range(1,7),inertias)
plt.scatter(x,y,c=kmeans.labels_)
# plt.plot(range(1,7),inertias,marker='x')
plt.xlabel("Value for k")
plt.ylabel("Inertia")
plt.show()
-----------------------------------------------
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris

irisData=load_iris()
X=irisData.data
y=irisData.target

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)
from sklearn.metrics import confusion_matrix
knn=KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train,y_train)
y_pred=knn.predict(X_test)
print(confusion_matrix(y_test,y_pred))
print("Accuracy: ",knn.score(X_test,y_test)*100)
--------------------------------------------------
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
data=pd.read_csv("/content/drive/MyDrive/ml/data4_19.csv")
attributes=data.iloc[:,:4].values
labels=data.iloc[:,4].values
dtree=DecisionTreeClassifier()
dtree.fit(attributes,labels)
features=['sepal-length','sepal-width','petal-length','petal-wdith']
tree.plot_tree(dtree,feature_names=features)
new_flower = [[5.2, 3.1, 1.4, 0.2]]
predicted_species = dtree.predict(new_flower)

print("Predicted Species:", predicted_species)
--------------------------------------------------
def find_s(data):
    hyp = ["0"]*(len(data[0][:-1]))
    
    for point  in data:
        if point[-1] == "1":
            for i in range(len(data[0][:-1])):
                if(hyp[i]=="0"):
                    hyp[i] = point[i]
                if(point[i] != hyp[i]):
                    hyp[i] = "?"
    return hyp      

training_data = [
    ["Sunny", "Warm", "Normal", "Strong", "Warm", "Same", "1"],
    ["Sunny", "Warm", "High", "Strong", "Warm", "Same", "1"],
    ["Rainy", "Cold", "High", "Weak", "Warm", "Change", "0"],
    ["Sunny", "Warm", "High", "Strong", "Cool", "Change", "1"]
]
find_s(training_data)
---------------------------------------------------
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler

iris = load_iris()
X = iris.data
y = iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

mlp = MLPClassifier(hidden_layer_sizes=(8, 4), activation='logistic', max_iter=10000, random_state=42)

mlp.fit(X_train, y_train)

predictions = mlp.predict(X_test)

accuracy = accuracy_score(y_test, predictions)
print(f"Accuracy: {accuracy}")
------------------------
import numpy as np

# Define the sigmoid activation function and its derivative
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Initialize weights and biases
w1, w2, w3, w4, w5, w6 = 0.7, -0.2, -0.4, 0.3, 0.5, 0.1
b1, b2, b3 =-0.4, 0.2, 0.1
learning_rate = 0.9
input_data = np.array([-1, 1])
target_output = 1

# Number of iterations
max_iterations = 10000
min_error = 1e-5

# Perform backpropagation
for iteration in range(max_iterations):
    # Forward Pass
    z1 = (input_data[0] * w1) + (input_data[1] * w2) + b1
    a1 = sigmoid(z1)
    z2 = (input_data[0] * w3) + (input_data[1] * w4) + b2
    a2 = sigmoid(z2)
    z_out = (a1 * w5) + (a2 * w6) + b3
    a_out = sigmoid(z_out)

    # Error calculation
    error = 0.5 * (target_output - a_out) ** 2

    # Backward Pass
    delta_out = -(target_output - a_out) * a_out * (1 - a_out)
    delta_a1 = delta_out * w5 * a1 * (1 - a1)
    delta_a2 = delta_out * w6 * a2 * (1 - a2)

    # Update weights and biases
    w5 -= learning_rate * a1 * delta_out
    w6 -= learning_rate * a2 * delta_out
    w1 -= learning_rate * input_data[0] * delta_a1
    w2 -= learning_rate * input_data[1] * delta_a1
    w3 -= learning_rate * input_data[0] * delta_a2
    w4 -= learning_rate * input_data[1] * delta_a2
    b1 -= learning_rate * delta_a1
    b2 -= learning_rate * delta_a2
    b3 -= learning_rate * delta_out

    # Check for convergence
    if error < min_error:
        print(f"Converged after {iteration + 1} iterations. Final error: {error}")
        break

# Print the final weights and biases
print(f"Final w1: {w1}")
print(f"Final w2: {w2}")
print(f"Final w3: {w3}")
print(f"Final w4: {w4}")
print(f"Final w5: {w5}")
print(f"Final w6: {w6}")
print(f"Final b1: {b1}")
print(f"Final b2: {b2}")
print(f"Final b3: {b3}")
----------------------------------
