1. Initialize h to the most specific hypothesis in H
2. For each positive training instance x
    For each attribute constraint a, in h
        If the constraint a, is satisfied by x
        Then do nothing
        Else replace a, in h by the next more general constraint that is satisfied by x
3. Output hypothesis h 
----------------------------------------
find xmean,ymean,x-xmean,y-ymean,x-xmean * y-ymean,x-xmean^2
sxy = x-xmean * y-ymean/n-1, sx^2 = x-xmean^2/n-1, b1=sxy/sx^2, b0=ymean-b1xmean
matrix: x = [1 x1] y = [y1] a=((xtx)^-1xt)y
            [1 x2]     [y2]
            [1 .]      [.]
----------------------------------------
Initialize k means with random values

--> For a given number of iterations:
    
    --> Iterate through items:
    
        --> Find the mean closest to the item by calculating 
        the euclidean distance of the item with each of the means
        
        --> Assign item to mean
        
        --> Update mean by shifting it to the average of the items in that cluster
------------------------------------------
KNN Algorithm:
Input:

Training dataset with labeled examples.
A new, unlabeled data point to classify or predict.
Choose the Number of Neighbors (k):

Decide on the number of neighbors (k) to consider. This is typically an odd number to avoid ties in classification.
Calculate Distance:

Measure the distance between the new data point and all points in the training set. Common distance metrics include Euclidean distance, Manhattan distance, or others depending on the problem.
Find k Nearest Neighbors:

Identify the k training examples with the smallest distances to the new data point.
Majority Vote (Classification) or Average (Regression):

For classification, assign the class label that is most common among the k neighbors.
For regression, predict the average of the target values of the k neighbors.
Output:

The predicted class label (for classification) or predicted value (for regression).
---------------------------------------
The ID3 Algorithm

. If all examples have the same label:

return a leaf with that label

Else if there are no features left to test:

return a leaf with the most common label

. Else:

choose the feature that maximises the information gain of S to be the next node using Equation (12.2)

- add a branch from the node for each possible value f in F

for each branch:

calculate S, by removing from the set of features

* recursively call the algorithm with Sr. to compute the gain relative to the current set of examples
------------------------------------------
1. Linear regression is a statistical method used for modeling the relationship between a dependent variable (also called the target or response variable) and one or more independent variables (predictors or features). It assumes that there is a linear relationship between the variables. The basic idea behind linear regression is to find the best-fit line that minimizes the sum of the squared differences between the observed and predicted values.

2. K-Nearest Neighbors (KNN) is a simple and versatile supervised machine learning algorithm used for classification and regression tasks. KNN relies on a distance metric. For classification tasks, the majority class among the K-nearest neighbors is assigned to the data point being classified.

3. K-Means is a popular unsupervised machine learning algorithm used for clustering, which is the process of grouping similar data points together. The algorithm aims to partition the data into K clusters, where each cluster is defined by its centroid

A Multilayer Perceptron (MLP) is a type of artificial neural network with multiple layers, including an input layer, one or more hidden layers, and an output layer. 

Components: Input layer, hidden layer, output layer, weights, biases, activation function


Forward Propagation:
Input values are passed through the network to compute the predicted output.
Loss Calculation:
The difference between the predicted output and the actual target (ground truth) is calculated using a loss function.
Backpropagation:
The error is propagated backward through the network to update the weights and biases. This is done using optimization algorithms like gradient descent.
Optimization:
Adjust the weights and biases to minimize the loss. This process is typically performed iteratively over the training data.
Epochs:
Training continues for multiple epochs until the model converges to a satisfactory level of performance.
-----------------
